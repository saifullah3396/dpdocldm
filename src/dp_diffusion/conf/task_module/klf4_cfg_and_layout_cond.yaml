defaults:
- layout_conditioned_latent_diffusion_local_vae@_here_
- _self_

# THIS IS NAMED CFG BUT ITS ACTUALLY CLASS_CONDITIONAL
loss_type: l2
enable_xformers_memory_efficient_attention: true
gradient_checkpointing: true
objective: ${objective}
diffusion_steps: 1000
inference_diffusion_steps: 200
scheduler: diffusers.schedulers.scheduling_ddpm.DDPMScheduler
# scheduler: diffusers.schedulers.scheduling_ddim.DDIMScheduler
noise_schedule: ${noise_schedule}
snr_gamma: ${snr_gamma}
clip_sample: false
clip_sample_range: 1.0
unnormalize_output: true
# class conditioning args
enable_class_conditioning: True
use_fixed_class_labels: True
use_cfg: False
cond_drop_prob: 0.1
guidance_scale: 1.0
custom_generated_class_label: null
use_precomputed_latents_if_available: True
compute_scale_factor: True
torch_model_builder:
  vae:
    model_name: atria.models.autoencoding.compvis_vae.CompvisAutoencoderKL
    pretrained: false
    is_frozen: true
    strict: false
checkpoint_configs:
  - checkpoint_path: pretrained_models/klf4_pretrained_iitcidp_133000.pt
    checkpoint_state_dict_path: ema_model.encoder_decoder_model
    model_state_dict_path: "non_trainable_models.vae"
    load_checkpoint_strict: True

